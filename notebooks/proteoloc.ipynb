{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9440/2486297870.py:17: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with hydra.initialize_config_dir(config_dir=config_dir):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Using  /home/ec2-user/outputs/proteoloc/2023-09-09/17-42-50/checkpoints/epoch=12-step=525.ckpt\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from glob import  glob\n",
    "from proteoscope.data import ProteolocDM\n",
    "from proteoscope.modules import ProteolocLM\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "BASE_DIR = \"/home/ec2-user/outputs/proteoloc/2023-09-09/17-42-50\" # Best proteoscope - ESM-full\n",
    "\n",
    "# BASE_DIR = \"/home/ec2-user/outputs-proteoscope/2023-08-04/16-34-38\" # 25 x 25 cond latent\n",
    "# BASE_DIR = \"/home/ec2-user/outputs-proteoscope/2023-08-04/22-36-50\" # 25 x 25 cond nuclei + latent\n",
    "# BASE_DIR = \"/home/ec2-user/outputs-proteoscope/2023-08-05/01-50-26\" # 25 x 25 cond nuclei + latent\n",
    "\n",
    "config_dir = BASE_DIR + \"/.hydra\"\n",
    "\n",
    "with hydra.initialize_config_dir(config_dir=config_dir):\n",
    "    config = hydra.compose(config_name=\"config\", overrides=OmegaConf.load(config_dir + \"/overrides.yaml\"))\n",
    "\n",
    "    chkpts = glob(BASE_DIR + \"/checkpoints/*.ckpt\")\n",
    "    chkpts.sort()\n",
    "    chkpt = chkpts[-2]\n",
    "    print('   Using ', chkpt)\n",
    "\n",
    "\n",
    "    pdm = ProteolocDM(\n",
    "        labels_path=config.data.labels_path,\n",
    "        sequences_path=config.data.sequences_path,\n",
    "        batch_size=config.trainer.batch_size,\n",
    "        num_workers=config.trainer.num_workers,\n",
    "        sequence_embedding=config.data.sequence_embedding,\n",
    "    )\n",
    "    pdm.setup()\n",
    "\n",
    "    plm = ProteolocLM.load_from_checkpoint(\n",
    "        chkpt,\n",
    "        module_config=config.module,\n",
    "    )\n",
    "\n",
    "    plm.eval()\n",
    "    plm.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = pdm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:15<00:00,  5.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import  tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for batch in tqdm(dl):\n",
    "    batch['sequence_embed'] = batch['sequence_embed'].to('cuda')\n",
    "    batch['sequence_mask'] = batch['sequence_mask'].to('cuda')    \n",
    "    batch['truncation'] = batch['truncation'].to('cuda')    \n",
    "    logits = plm(batch)\n",
    "    prediction = torch.argmax(logits, -1)\n",
    "    predicted_labels.append(prediction.detach().cpu().numpy())\n",
    "    true_labels.append(batch['localization'].detach().cpu().numpy())\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "predicted_labels = np.concatenate(predicted_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8200504868373603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "acc = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Accuracy {acc}')\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa316eecd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cm / cm.sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on OpenCell protein embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# seq_path = '/home/ec2-user/cytoself-data/sequences.csv'\n",
    "# seq = pd.read_csv(seq_path, index_col=0)\n",
    "# # seq['Length'] = seq[\"Peptide\"].apply(lambda x: len(x.replace(\"*\", \"\")))\n",
    "# # seq.to_csv(seq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plds = ProteolocDM(\n",
    "    labels_path='/home/ec2-user/cytoself-data/sequences.csv',\n",
    "    sequences_path=None, #'/home/ec2-user/cytoself-data/ESM_sequence_embeddings_full.zarr',\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    sequence_embedding=None, #'ESM-full',\n",
    ")\n",
    "plds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1311/1311 [02:28<00:00,  8.81it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 377 and the array at index 1 has size 340",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ec2-user/proteoscope/notebooks/proteoloc.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baws-ec2-3/home/ec2-user/proteoscope/notebooks/proteoloc.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         seq_embeds \u001b[39m=\u001b[39m plm\u001b[39m.\u001b[39membed(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baws-ec2-3/home/ec2-user/proteoscope/notebooks/proteoloc.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         embeds\u001b[39m.\u001b[39mappend(seq_embeds\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baws-ec2-3/home/ec2-user/proteoscope/notebooks/proteoloc.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m embeds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(embeds, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 377 and the array at index 1 has size 340"
     ]
    }
   ],
   "source": [
    "from tqdm import  tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "embeds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(plds.predict_dataloader()):\n",
    "        # batch['sequence_embed'] = batch['sequence_embed'].to('cuda')\n",
    "        # batch['sequence_mask'] = batch['sequence_mask'].to('cuda')    \n",
    "        batch['truncation'] = batch['truncation'].to('cuda')    \n",
    "        seq_embeds = plm.embed(batch)\n",
    "        embeds.append(seq_embeds.detach().cpu().numpy())\n",
    "embeds = np.concatenate(embeds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import os\n",
    "\n",
    "\n",
    "PROTEIN_EMBED_PATH = '/home/ec2-user/cytoself-data/ESM_sequence_embeddings_full_lora2.zarr'\n",
    "\n",
    "z_embedding_prot = zarr.open(\n",
    "    PROTEIN_EMBED_PATH,\n",
    "    mode=\"w\",\n",
    "        shape=(len(embeds), 1024 + 1, X.shape[2]),\n",
    "        chunks=(1, None, None),\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, zz in enumerate(embeds):\n",
    "    ll = min(1024, len(zz[0]))\n",
    "    z_embedding_prot[i, 1:1+ll, :] = zz[0][:ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 459, 1280)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1025, 1280)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_embedding_prot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\", \"fc1\", \"fc2\"], inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,082,560 || all params: 657,125,814 || trainable%: 0.9256309629619877\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 1280])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM2(\n",
      "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (16): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (17): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (18): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (19): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (20): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (21): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (22): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (23): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (24): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (25): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (26): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (27): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (28): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (29): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (30): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (31): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (32): TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (contact_head): ContactPredictionHead(\n",
      "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "    (activation): Sigmoid()\n",
      "  )\n",
      "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106676/968085745.py:5: DtypeWarning: Columns (17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = '/home/ec2-user/cytoself-data/labels.csv'\n",
    "data_path2 = '/home/ec2-user/cytoself-data/sequences.csv'\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df2 = pd.read_csv(data_path2, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['loc'] = df2['localization'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(data_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
