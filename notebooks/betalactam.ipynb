{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdrug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdrug.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import datasets\n",
    "\n",
    "BASE_PATH = \"/home/ec2-user/esm/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:34:05   Downloading https://miladeepgraphlearningproteindata.s3.us-east-2.amazonaws.com/peerdata/beta_lactamase.tar.gz to /home/ec2-user/esm/beta_lactamase.tar.gz\n",
      "00:34:05   Extracting /home/ec2-user/esm/beta_lactamase.tar.gz to /home/ec2-user/esm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing proteins from sequences: 100%|██████████| 5198/5198 [00:07<00:00, 717.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import transforms\n",
    "from torchdrug import datasets\n",
    "\n",
    "truncate_transform = transforms.TruncateProtein(max_length=1024, random=False)\n",
    "protein_view_transform = transforms.ProteinView(view=\"residue\")\n",
    "transform = transforms.Compose([truncate_transform, protein_view_transform])\n",
    "# dataset = datasets.SubcellularLocalization(SUBCELLULAR_PATH, atom_feature=None, bond_feature=None, residue_feature=\"default\", transform=transform)\n",
    "dataset = datasets.BetaLactamase(BASE_PATH, atom_feature=None, bond_feature=None, residue_feature=\"default\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': Protein(num_atom=0, num_bond=0, num_residue=286),\n",
       " 'scaled_effect1': 0.9426838159561157}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of first sample:  0.9426838159561157\n",
      "train samples: 4158, valid samples: 520, test samples: 520\n"
     ]
    }
   ],
   "source": [
    "print(\"The label of first sample: \", dataset[0][dataset.target_fields[0]])\n",
    "print(\"train samples: %d, valid samples: %d, test samples: %d\" % (len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_seq = dataset[0]['graph'].to_sequence().replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4158/4158 [00:02<00:00, 1945.37it/s]\n",
      "100%|██████████| 520/520 [00:00<00:00, 1951.91it/s]\n",
      "100%|██████████| 520/520 [00:00<00:00, 1920.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "seq = []\n",
    "for item in tqdm(train_set):\n",
    "    aa = item['graph'].to_sequence().replace('.', '')\n",
    "    lf = item['scaled_effect1']\n",
    "    seq.append({'seq': aa, 'loc': lf, 'split': 'train'})\n",
    "\n",
    "for item in tqdm(valid_set):\n",
    "    aa = item['graph'].to_sequence().replace('.', '')\n",
    "    lf = item['scaled_effect1']\n",
    "    seq.append({'seq': aa, 'loc': lf, 'split': 'val'})\n",
    "\n",
    "for item in tqdm(test_set):\n",
    "    aa = item['graph'].to_sequence().replace('.', '')\n",
    "    lf = item['scaled_effect1']\n",
    "    seq.append({'seq': aa, 'loc': lf, 'split': 'test'})\n",
    "\n",
    "seq = pd.DataFrame(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.to_csv('protein_scaled_effect1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train flourescence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seq = pd.read_csv('protein_scaled_effect1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLILTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq['seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "conversion = 'ARNDCQEGHILKMFPSTWYVX'\n",
    "amino_acids = np.array([a for a in conversion])\n",
    "aa = seq['seq'].iloc[0]\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories=[amino_acids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = seq['seq'].iloc[0]\n",
    "sequence_array = np.array(list(sequence)).reshape(-1, 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(sequence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5198it [00:04, 1145.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "embed = np.zeros((len(seq), 286, 21))\n",
    "\n",
    "for i, sequence in tqdm(enumerate(seq['seq'])):\n",
    "    sequence_array = np.array(list(sequence)).reshape(-1, 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(sequence_array)\n",
    "    embed[i, :onehot_encoded.shape[0]] = onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embed.reshape(len(seq), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5198, 6006)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (seq['split'] == 'train').values\n",
    "X_train = embed[index]\n",
    "y_train = seq[index]['loc']\n",
    "\n",
    "index = (seq['split'] == 'val').values\n",
    "X_val = embed[index]\n",
    "y_val = seq[index]['loc']\n",
    "\n",
    "index = (seq['split'] == 'test').values\n",
    "X_test = embed[index]\n",
    "y_test = seq[index]['loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4158, 6006)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = 0.5\n",
    "clf = Ridge(alpha=alpha)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.7245350761323257, pvalue=9.098781282748791e-86)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the learned parameters\n",
    "weights = clf.coef_\n",
    "intercept = clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parameters\n",
    "np.save('weights_beta.npy', weights)\n",
    "np.save('intercept_beta.npy', intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the parameters\n",
    "weights = np.load('weights_beta.npy')\n",
    "intercept = np.load('intercept_beta.npy')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "weights_torch = torch.from_numpy(weights)\n",
    "intercept_torch = torch.from_numpy(np.array([intercept]))\n",
    "\n",
    "# Define a linear layer\n",
    "linear_layer = torch.nn.Linear(weights.shape[0], 1)\n",
    "\n",
    "# Set the weights and bias\n",
    "with torch.no_grad():  # We don't want these operations to be tracked by the autograd\n",
    "    linear_layer.weight.data = weights_torch\n",
    "    linear_layer.bias.data = intercept_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=6006, out_features=1, bias=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# TEMPLATE CLASS\n",
    "class Potential:\n",
    "    \n",
    "    def get_gradients(seq):\n",
    "        '''\n",
    "            EVERY POTENTIAL CLASS MUST RETURN GRADIENTS\n",
    "        '''\n",
    "        \n",
    "        sys.exit('ERROR POTENTIAL HAS NOT BEEN IMPLEMENTED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFP_beta_lactam(Potential):\n",
    "    \"\"\"\n",
    "    Potential for beta lactam\n",
    "    \"\"\"    \n",
    "    def __init__(self, args, features, potential_scale, DEVICE):\n",
    "        weights = args['weights']\n",
    "        intercept = args['intercept']\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        weights_torch = torch.from_numpy(weights)\n",
    "        intercept_torch = torch.from_numpy(np.array([intercept]))\n",
    "\n",
    "        # Define a linear layer\n",
    "        linear_layer = torch.nn.Linear(weights.shape[0], 1)\n",
    "\n",
    "        # Set the weights and bias\n",
    "        with torch.no_grad():  # We don't want these operations to be tracked by the autograd\n",
    "            linear_layer.weight.data = weights_torch\n",
    "            linear_layer.bias.data = intercept_torch        \n",
    "        \n",
    "        self.linear_layer = linear_layer.to(DEVICE)\n",
    "        self.potential_scale = potential_scale\n",
    "        self.sequence_length = 286\n",
    "        \n",
    "    def get_gradients(self, seq):\n",
    "        \"\"\"\n",
    "        Calculate gradients with respect to activity\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        seq : tensor\n",
    "            L X 21 logits after saving seq_out from xt\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradients : list of tensors\n",
    "            gradients of seq with respect to flourescence\n",
    "        \"\"\"\n",
    "        soft_seq = torch.softmax(seq, dim=1)\n",
    "\n",
    "        if soft_seq.shape[0] > self.sequence_length:\n",
    "            soft_seq = soft_seq[:self.sequence_length]\n",
    "\n",
    "        if soft_seq.shape[0] < self.sequence_length:\n",
    "            zeros = torch.zeros(self.sequence_length - soft_seq.shape[0], soft_seq.shape[1])\n",
    "            soft_seq = torch.cat((soft_seq, zeros), 0)\n",
    "\n",
    "        score = linear_layer(soft_seq.reshape(-1))\n",
    "        score.backward()\n",
    "        gradients = soft_seq.grad\n",
    "\n",
    "        return gradients * self.potential_scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
