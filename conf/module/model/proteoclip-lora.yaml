image_embedding_dims: 2500
protein_embedding_dims: 1280
projection_dims: 1024
dropout: 0.2
bias: 0.3
esm:
  d_model: 1280 # cross_attention_dim 1280 2560
  model_name: 'esm2_t33_650M_UR50D' # esm2_t33_650M_UR50D esm2_t36_3B_UR50D
  embedding_layer: 33 # 33 36
  truncation_seq_length: 1024
  lora:
    target_modules:
      - "k_proj"
      - "v_proj"
      - "q_proj"
      - "out_proj"
    inference_mode: False
    r: 4
    lora_alpha: 4
    lora_dropout: 0.1
esm_bottleneck:
  d_init: 1280
  d_model: 0 # cross_attention_dim
  nhead: 4
  num_encoder_layers: 0
  dim_feedforward: 256
  dropout: 0.2
  return_type: 'full'
autoencoder:
  checkpoint: /home/ec2-user/outputs/autoencoder/2023-08-19/17-28-45/checkpoints/last.ckpt # 2023-08-17/05-03-37 2023-08-19/17-28-45
  model:
    image_height: 100
    in_channels: 2
    out_channels: 2
    layers_per_block: 2
    block_out_channels: [128, 256, 512]
    latent_channels: 4
    down_block_types: ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D']
    up_block_types: ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']
    classifier: null
    loss:
      classifier_weight: 0.1
      kl_weight: 1e-7
      perceptual_weight: 2.0
      pixel_weight: 1.0
      disc_weight: 1.0
      disc_start: 2400